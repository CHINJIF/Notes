{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hadoop\n",
    "#### 起源\n",
    "google搜索引擎\n",
    "* google集装箱数据中心，每个集装箱里又1160台服务器，标准化。能效比1.25.一般公司在2.0\n",
    "* 解决问题：\n",
    "    * 怎么存储大量网页，数据冗余，保证安全\n",
    "    * 搜索算法，如果要模糊匹配 LIKE xxx, 在SQL中是不能用索引的，而是要全表扫描，慢（用倒排索引）\n",
    "    * page-rank计算问题\n",
    "\n",
    "#### 倒排索引\n",
    "```\n",
    "word       index\n",
    "Google    {1:1}, {3:2}      // documentId: offset\n",
    "```\n",
    "* 分词 -> 去掉stopword -> 倒排索引\n",
    "\n",
    "##### 分词\n",
    "* 一种办法，用字典。 切出一个词，在字典里查看有没有\n",
    "\n",
    "#### page-rank\n",
    "* 垃圾里面找黄金\n",
    "* 页面的重要性如何判断？\n",
    "    * 点击量 -- google爬虫爬不到\n",
    "    * 通过被引用数量判断，如果被引用得多就证明该页面重要\n",
    "    * 被page-rank高的页面指向，该页面重要性更高\n",
    "* google matrix：\n",
    "\n",
    "```\n",
    "\n",
    "1 -> 2\n",
    "|  x |\n",
    "3 -> 4\n",
    "\n",
    "0    0    0  0\n",
    "1/3  0    0  1\n",
    "1/3  1/2  1  0\n",
    "1/3  1/2  0  0\n",
    "\n",
    "G = aS + (1-a)1/n * U\n",
    "q=Gq, 求上特征矩阵的特征向量。特征向量q[i]就是rank-score\n",
    "```\n",
    "通过不断的map-reduce，来进行矩阵的运算，最后出一个千上万列的matrix的特征向量\n",
    "\n",
    "#### Google tech key\n",
    "* GFS\n",
    "* Map-reduce\n",
    "* Big table\n",
    "\n",
    "#### lucene\n",
    "* Hadoop的源起 -- Lucene\n",
    "* java 做的全文检索工具\n",
    "* lucene的微缩版：Nutch\n",
    "\n",
    "### Hadoop子项目\n",
    "* HBASE, PIG, HIVE\n",
    "* MapReduce, HDFS, ZooKeeper\n",
    "* Core, Avro\n",
    "\n",
    "### 架构\n",
    "* JobTracker, TaskTracker\n",
    "* NameNode, DataNode\n",
    "\n",
    "##### HDFS\n",
    "* NameNode - Master\n",
    "   * HDFS的守护程序\n",
    "   * 作为分布式文件系统的总控作用\n",
    "   * 对内存和I/O进行集中管理\n",
    "   * 是单点，有单点故障的风险\n",
    "* Secondary NameNode 辅助名称节点\n",
    "   * 作为NameNode的备份\n",
    "   * 与NameNode通讯，对NameNode快照\n",
    "* DataNode - Slave\n",
    "   * 数据分布式存储\n",
    "\n",
    "##### MapReduce\n",
    "* JobTracker - Master\n",
    "   * 用于处理作业(用户提交代码)的后台程序\n",
    "   * 决定那些文件参与处理，然后切割task并分配节点\n",
    "   * 监控task，重启失败的task\n",
    "   * 每个集群只有唯一的一个JobTracker，位于master节点\n",
    "* TaskTrakcer - Slave\n",
    "   * 位于Slave节点和与datanode结合\n",
    "   * 管理个子节点的task，由jobTracker分配\n",
    "   * 每个节点自会有一个taskTracker，可以启动多个JVM\n",
    "\n",
    "#### 安装使用\n",
    "skip\n",
    "##### 测试\n",
    "\n",
    "```\n",
    "> bin/hadoop dfs -put ../intput in \n",
    "> bin/hadoop dfs -ls ./in/*           // hadoop 分布式文件操作命令\n",
    "```\n",
    "\n",
    "```\n",
    "> bin/hadoop jar hadoop-0.20.2-example.jar wordcount in out     //运行一个叫wordcount的作业\n",
    "```\n",
    "\n",
    "```\n",
    "> bin/hadoop dfs -ls ./out\n",
    "  /user/grid/out/_logs\n",
    "  /user/grid/out/part-r-00000\n",
    "```\n",
    "\n",
    "* 通过浏览器检查jobtracker在结点50030端口，监控jobtracker\n",
    "   * http://localhost:50030/jobtracker.jsp\n",
    "   * list: complete jobs/filed jobs/running jobs/local logs\n",
    "   * job details: file link, running time, etc\n",
    "* 通过浏览器访问namenode所在结点在50070端口监控集群\n",
    "   * browse filesystem\n",
    "   * cluster summary\n",
    "   * namenode storage\n",
    "   * logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS\n",
    "#### 物理存储\n",
    "* 每个服务器里有路径\n",
    "   * blk—34820934092830\n",
    "   * blk—34820934092830_1037.meta\n",
    "\n",
    "#### 设计思想\n",
    "* 硬件错误是常态性的，需要冗余\n",
    "* 流式数据方位，支持数据的批量读取，而**非随机读取**，hadoop擅长**数据分析**而**不**是**事务处理**。NOT OLTP, OLAP\n",
    "* 简单的一致性模型，为了降低系统复杂性，对文件采取一次性写，多次读。文件一经写入，无法修改\n",
    "* 程序采用“数据就进”的原则分配节点执行\n",
    "\n",
    "#### 体系结构\n",
    "* Namenode\n",
    "   * 记录文件数据块在哪个datanode的位置和副本信息\n",
    "   * 元数据操作：事务日志\n",
    "   * 元数据操作：映射文件\n",
    "* Datanode\n",
    "   * 一次写入，多次读取，不修改\n",
    "   * 文件由数据块组成，一般64MB\n",
    "* 事务日志\n",
    "* 映像文件\n",
    "* Secondary Namenode\n",
    "\n",
    "#### 可靠性\n",
    "* 冗余副本策略\n",
    "   * hdfs—site.xml设置复制因子，指定副本数量\n",
    "* 机架策略\n",
    "   * 集群放在不同的机架上，连在同一个交换机上，机架内带宽小\n",
    "   * HDFS的机架感知\n",
    "   * 本机架一个副本，别的一个副本\n",
    "* 心跳机制\n",
    "   * Namenode接收datanode的心跳和块报告\n",
    "* 安全模式\n",
    "   * Namenode在启动时会经过一个“安全模式“\n",
    "* 校验和\n",
    "   * 文件创立时都有checksum\n",
    "   * 校验和会存储在一个隐藏文件夹里用于验证\n",
    "* 回收站\n",
    "   * 文件不会立马删除，可以快速恢复\n",
    "* 元数据保护\n",
    "   * 映像文件 + 事务日志是namenode的核心数据\n",
    "   * 增加映像文件 + 事务日志的备份\n",
    "* 快照\n",
    "\n",
    "### HDFS文件操作\n",
    "* 命令行方式\n",
    "* API方式\n",
    "\n",
    "###### 列出目录\n",
    "* HDFS里面没有当前目录的概念，只能ls，不能cd\n",
    "```\n",
    "> bin/hadoop dfs -ls ./in/\n",
    "```\n",
    "\n",
    "###### 上传文件\n",
    "* HDFS里面没有当前目录的概念，只能ls，不能cd\n",
    "```\n",
    "> bin/hadoop dfs -put path target_path\n",
    "```\n",
    "\n",
    "###### 复制到本地\n",
    "```\n",
    "> bin/hadoop dfs -get source_path output_path\n",
    "```\n",
    "\n",
    "###### 删除\n",
    "```\n",
    "> bin/hadoop dfs -rmr path\n",
    "```\n",
    "\n",
    "###### 查看内容\n",
    "```\n",
    "> bin/hadoop dfs -cat path\n",
    "```\n",
    "\n",
    "###### 查看多个文件\n",
    "```\n",
    "> zcat *.gz > abc       // 合并多个小文件到一个\n",
    "``` \n",
    "\n",
    "###### 查看管理信息\n",
    "```\n",
    "> bin/hadoop dfsadmin -report\n",
    "```\n",
    "\n",
    "###### 安全模式\n",
    "```\n",
    "> bin/hadoop dfsadmin -safemode enter\n",
    "```\n",
    "\n",
    "#### Hadoop管理节点\n",
    "##### 增加namenode\n",
    "* 新机器安装好hadoop\n",
    "* 复制namenode的配置文件复制到该节点\n",
    "* 修改masters，slaves文件，增加该节点\n",
    "* hadoop-daemon.sh start datanode/tasktracker\n",
    "* 运行start-balancer.sh进行负载均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce\n",
    "并行计算的框架\n",
    "\n",
    "### 示例，气象局\n",
    "* map: 每一条数据，map 提取年份操作，和对应气温\n",
    "* shuffle：按年份聚合 (该步骤可选), shuffle 可以分摊 reduce阶段需要的计算量，一般用于reduce的机器数量很少\n",
    "* reduce： 求每一年的最高气温 (也可以不需要reduce，直接写出数据就可以\n",
    "\n",
    "\n",
    "* **mapper，reducer可以不同组合**\n",
    "* **可以分治的算法就可以map-reduce**， map对应分，reduce\n",
    "* reducer 的数量可以配置\n",
    "* shuffler 在reduce之前对数据进行预处理和压缩， 减少了需要传输给reducer的**网络IO**\n",
    "![HBASE Map Reduce](img/hadoop_map_reduce.png)\n",
    "\n",
    "#### 代码例子\n",
    "##### Mapper\n",
    "```java\n",
    "public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "    \n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "    \n",
    "    public void map(Object key, Text value, Context context) throws IOException, InterruptedException{\n",
    "        System.out.println(\"key= \" +  key.toString());\n",
    "        System.out.println(\"value= \" + value.toString());\n",
    "    \n",
    "        StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "        while(itr.hasMoreTokens()) {\n",
    "            word.set(itr.nextToken());\n",
    "            context.write(word, one);\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "* 继承java里的Mapper类，然后实现map方法\n",
    "\n",
    "##### Reducer\n",
    "```java\n",
    "public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    ...\n",
    "    public void reduce(){};\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "##### 组合调用 main\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n",
    "    \n",
    "    Job job = new Job(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValeClass(IntWritable.class);\n",
    "    \n",
    "    job.waitForCompletion(true);\n",
    "}\n",
    "```\n",
    "* 设置mapper，reducer，shuffer，output，提交作业\n",
    "\n",
    "##### 性能调优\n",
    "* 究竟需要多少reducer\n",
    "* 输入：大文件由于小文件\n",
    "* 减少网络IO: 压缩map的输出\n",
    "\n",
    "\n",
    "#### 调度机制\n",
    "* FIFO\n",
    "* 支持公平调度\n",
    "* 支持容量调度\n",
    "\n",
    "### 工作机制\n",
    "![HBASE Map Reduce](img/map_reduce_architecture.png)\n",
    "\n",
    "#### 错误处理机制\n",
    "##### 硬件故障\n",
    "* 单点故障：jobtracker，namenode，选最牢靠的硬件\n",
    "* jobtracker 通过 heartbeat(周期1min)监控task tracker。\n",
    "* 如果没有发心跳 \n",
    "    * 可能时硬件故障\n",
    "    * 也可能负载过重，没来得及发\n",
    "* 发现没有心跳：\n",
    "    * 如果故障节点在执行map任务且没有完成，jobtracker会要求其他节点**重新执行**\n",
    "    * 如果在执行reduce，尚未完成，jobtracker会要求其他节点**继续执行**\n",
    "    \n",
    "##### 软件故障，任务失败\n",
    "* 代码缺陷引起的进程崩溃\n",
    "* jvm自动退出，向tasktracker发送错误信息，错误信息写入日志\n",
    "* tasktracker监听程序发现进程退出，标记任务失败\n",
    "* 标记任务失败，把失败任务重新放入调度队列\n",
    "* 如果失败超过阈值（4次），将不会被执行，任务宣告失败"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运维\n",
    "* 审计log\n",
    "\n",
    "* 对hadoop集群进行监控\n",
    "    * Ganglia\n",
    "    * openstack\n",
    "    * chukwa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 架构师\n",
    "* 优缺点\n",
    "* 瓶颈\n",
    "* 使用场景\n",
    "* 使用成本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBASE\n",
    "\n",
    "\n",
    "## Pig\n",
    "* pig 是hadoop的客户端\n",
    "* 提供类似于SQL语句的面向数据流语言\n",
    "* pig latin 可以进行排序，过滤，求和，分组，关联， 自定义函数。是一种面向数据分析处理的轻量级脚本语言\n",
    "* pig 可以看作是 pig latin 到 map-reduce的映射器\n",
    "\n",
    "## Hive\n",
    "* 把SQL语句映射为mapReduce的查询\n",
    "\n",
    "\n",
    "## YARN\n",
    "* Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源\n",
    "* Yarn相当与一个**分布式操作系统平台**，map reduce等运算程序相当于应用程序\n",
    "\n",
    "![HBASE Map Reduce](img/hadoop_yarn.png)\n",
    "\n",
    "https://www.bilibili.com/video/av38472787?from=search&seid=8620089201747655937\n",
    "\n",
    "#### hadoop 1.0\n",
    "利用JobTracker框架来进行调度\n",
    "\n",
    "* master/slave模式，利用jobTracker(master) 来调度taskTracker(slave) 来干活\n",
    "* JobTracker：\n",
    "    * 资源调度，任务监控（心跳）\n",
    "* TaskTracker:\n",
    "    * 监听自己机器的资源情况\n",
    "    * 监视当前机器的tasks运行\n",
    "    * 向JobTracker汇报\n",
    "* 缺点：\n",
    "    * 单点故障\n",
    "    * JobTracker最多只支持4000节点的主机\n",
    "    * 在TaskTracker端只是以简单的task数目来表示资源，而不是用cpu/内存的占用情况来表示\n",
    "    * 在taskTracker端强行把资源划分为了map task，reduce task slot 浪费资源\n",
    "    * JobTracker做的事情太多了，源代码复杂，增加维护成本\n",
    "    * 只能运行MapReduce\n",
    "\n",
    "#### hadoop 2.0\n",
    "![HBASE Map Reduce](img/yarn.png)\n",
    "\n",
    "* Resource Manager：\n",
    "    * RM, 负责集群的自愿的统一管理和调度，\n",
    "    * 处理客户端请求\n",
    "    * 监控集群中的NM\n",
    "* NodeManager：\n",
    "    * NM, 负责自己所在节点的应用资源使用情况，并且向RM汇报\n",
    "    * 接收并处理RM, AM的各种命令\n",
    "* App Master: 比作项目经理，来进行具体的需要多少人，怎么分工， 分摊了之前JobTracker的很多工作\n",
    "    * 每个应用程序对应一个AM, 负责应用程序管理\n",
    "    * 向RM申请资源，并分给task\n",
    "    * AM与NM通信，来启动或者停止task\n",
    "* Container：\n",
    "    * 封装了CPU,Memory等资源的容器\n",
    "    * 相当于是一个任务运行环境的抽象\n",
    "    * 网络io？\n",
    "    \n",
    "### 任务调度过程\n",
    "##### FIFO 调度器\n",
    "\n",
    "##### 容量调度器\n",
    "\n",
    "##### 公平调度器\n",
    "\n",
    "\n",
    "## Data serialization: Parquet, Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 锁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paxos 算法 选举功能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大规模部署集群\n",
    "https://www.bilibili.com/video/av23525072/?p=33\n",
    "\n",
    "### 应用\n",
    "https://www.bilibili.com/video/av23525072/?p=36\n",
    "* 作为存储集群，有冗余备份\n",
    "* map-reduce可以提供快速并行计算，可以进行数据分析\n",
    "\n",
    "##### 场景 1： 日志分析\n",
    "* CDN 加速技术，CDN服务器作为反向代理，还提供cache，分布在不同的物理节点，广州，上海 etc\n",
    "* CDN 通过分析用户日志来cache\n",
    "* 要排除日志里爬虫和程序的点击，e.g 用鼠标探测来反爬\n",
    "* 跟踪用户，cookie\n",
    "\n",
    "* 问题：\n",
    "    * 日志的保存消耗空间\n",
    "    * 日志需要备份\n",
    "    * 统计效率低，时滞明显，不满足业务要求\n",
    "\n",
    "###### 方案\n",
    "* Hadoop + HBASE:\n",
    "    * 设定数据定时过期，定期清理\n",
    "    * 利用PIG 查询统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络编程\n",
    "* 链接\n",
    "    * 长连接\n",
    "    * 短连接\n",
    "* 协议\n",
    "    * tcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异步编程\n",
    "* 多线程\n",
    "* 多进程\n",
    "* 锁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
